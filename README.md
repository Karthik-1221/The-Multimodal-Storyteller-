# The Multimodal Storyteller 

Welcome to The Multimodal Storyteller, an interactive web application that allows you to co-create unique sagas with artificial intelligence. Set the stage for your world, guide the narrative chapter by chapter, and watch your story come to life with AI-generated art and audio narration.

  <!-- It's a great idea to replace this with a real screenshot of your app! -->

##  Key Features

-   **Generative World-Building**: Establish the creative DNA of your universe by choosing a theme, a protagonist archetype, and a "strange contradiction" that makes your world unique.
-   **Interactive & Branching Narrative**: You write the opening line, and from there, the AI generates rich story chapters and presents you with three distinct choices to steer the plot.
-   **AI-Generated Art**: Each new chapter is accompanied by a unique image generated by Stable Diffusion XL, visually interpreting the scene you just read.
-   **Wildcard Story Twists**: To keep the story fresh and unpredictable, one of the three choices is always a "Wildcard" designed to subvert expectations and connect to your world's core contradiction.
-   **Text-to-Speech Narration**: Immerse yourself in the saga by listening to it with a built-in audio player that uses your browser's native speech synthesis.

##  How It Works: A Multi-Persona AI Engine

The application is powered by a sophisticated "master prompt" that instructs Google's Gemini model to act as a collaborative, multi-persona creative team.

1.  **Set the Stage**: You provide three creative inputs. The AI uses these to generate a secret **"World Bible"**‚Äîa foundational document containing the world's history, rules, and tone. This document is stored in the session and ensures narrative consistency.
2.  **Begin the Saga**: You write the opening sentence to set the initial direction.
3.  **Weave the Next Chapter**: For each new chapter, the AI performs three roles:
    -   **As a Literary Artist:** It writes the next rich, descriptive narrative chapter.
    -   **As a Plot Theorist:** It creates three new single-sentence choices for you, including the Wildcard.
    -   **As an Art Director:** It writes a concise, descriptive prompt for the image generator based on the new scene.
4.  **Paint the Scene**:Of course. Here is a comprehensive and well-structured `README.md` file for your Multimodal Storyteller project. It covers the what, why, and how, making it perfect for your GitHub repository.

 The art prompt is sent to the Hugging Face Inference API, which uses **Stable Diffusion XL** to create an image.
5.  **Repeat**: The loop continues as you make new choices, collaboratively building your unique, multimodal story.

## üõ†Ô∏è Tech Stack

-   **Frontend & Application Logic**: [Streamlit](https://streamlit.io/)
-   **Language**: Python
-   **LLM (Text & Logic Generation)**: Google Gemini 1.5 Flash (via `google-generativeai` SDK)
-   **Image Generation**: Stable Diffusion XL (via Hugging Face Inference API)
-   **API Key Management**: `python-dotenv` (local) & `st.secrets` (deployment)

## üöÄ Setup and Installation

Follow these steps to run the application on your local machine.

### 1. Prerequisites

-   Python 3.9+
-   A **Google API Key** from [Google AI Studio](https://aistudio.google.com/app/apikey).
-   A **Hugging Face API Token** from [Hugging Face](https://huggingface.co/settings/tokens) (a "Read" role is sufficient).

### 2. Clone the Repository

```bash
git clone https://github.com/Karthik-1221/The-Multimodal-Storyteller-.git
cd multimodal-storyteller
```


### 3. Set Up a Virtual Environment

It is highly recommended to use a virtual environment to manage project dependencies.

```bash
# Create a virtual environment
python -m venv venv

# Activate it
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate
```

### 4. Install Dependencies

Install all the required Python packages.

```bash
pip install -r requirements.txt
```

### 5. Configure API Keys (Local)

-   Create a new---

````markdown
# The Multimodal Storyteller 

The Multimodal Storyteller is an interactive web application that allows you to co-create unique sagas with a powerful AI ensemble. You don't just write a story; you build its universe, guide its narrative, and watch it come to life with AI-generated art and audio narration.

This project is built with Python and Streamlit, and it showcases a modern approach to controlling Large Language Models (LLMs) by instructing them to act as a team of creative experts.

 
*Note: You should replace the link above with a real screenshot of your running application.*

## Key Features

-   **Generative World-Building**: Don't start with a blank page. First, "Set the Stage" by defining your world's core theme, a protagonist archetype, and a strange, central contradiction. This creates a secret "World Bible" that gives your story a consistent and unique foundation.
-   **Interactive, Branching Narrative**: You write the opening line, and the AI takes it from there, writing the next chapter and then presenting you with three distinct paths forward.
-   **AI-Generated Art**: Every new chapter is accompanied by a unique image generated by Stable Diffusion XL, visually interpreting the scene you just read.
-   **"Wildcard" Choices**: To keep the story exciting and unpredictable, one of the three choices is always a "Wildcard" designed to subvert expectations or connect to your world's core contradiction.
-   **Text-to-Speech Narration**: Listen to your unfolding saga with a built-in audio player that uses your browser's native speech synthesis engine.

## How It Works: A Multi-Persona AI Ensemble

The core of this application is a sophisticated prompting technique that instructs a single LLM (Google's Gemini) to act as a team of three creative experts:

1.  **The Literary Artist:** Writes the next rich, descriptive chapter of the story, focusing on prose and atmosphere.
2.  **The Plot file in the project's root directory named `.env`.
-   Copy the contents of `.env.example` into your new `.env` file.
-   Add your secret API keys to the `.env` file:

    ```
    # .env file
    GOOGLE_API_KEY="PASTE_YOUR_GOOGLE_API_KEY_HERE"
    HF_API_TOKEN="PASTE_YOUR_HUGGING_FACE_TOKEN_HERE"
    ```

### 6. Run the Application

```bash
streamlit run app.py
```

Your browser should automatically open a new tab with the running application.

##  Deployment on Streamlit Community Cloud

1.  Push your project (including `app.py`, `requirements.txt`, etc.) to a public GitHub repository. **Do not commit your `.env` file!**
2.  Go to [share.streamlit.io](https://share.streamlit.io) and sign in.
3.  Click "New app" and choose your repository.
4.  In the "Advanced settings...", go to the "Secrets" section.
5.  Add your API keys in the TOML format. The variable names must be `GOOGLE_API_KEY` and `HF_API_TOKEN`.

    ```toml
    # Streamlit Secrets (secrets.toml)
    GOOGLE_API_KEY = "your_google_api_key_here"
    HF_API_TOKEN = "your_hugging_face_token_here"
    ```

6.  Click "Deploy!".

## ‚úçÔ∏è Author

Created by **Karthik**
